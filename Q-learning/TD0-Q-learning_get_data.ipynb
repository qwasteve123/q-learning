{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QLearn:\n",
    "    def __init__(self,env:gym.Env):\n",
    "        self.env = env\n",
    "        self.obs_space: gym.spaces.Box = env.observation_space \n",
    "\n",
    "    def set_num_of_increments(self,num_of_increments: np.ndarray):\n",
    "        self.num_of_increments: np.ndarray = num_of_increments\n",
    "        self.increment: np.ndarray = self._obs_increment()\n",
    "\n",
    "    \n",
    "    def _obs_increment(self) -> np.ndarray:\n",
    "        high = np.array([1.5, 1.5, 3, 2, 1.5, 3,1,1])\n",
    "        low = np.array([-1.5,-1.5,-3,-2,-1.5,-3,0,0])\n",
    "        # range = self.obs_space.high - self.obs_space.low\n",
    "        range = high - low\n",
    "        increment = range / self.num_of_increments\n",
    "        return increment\n",
    "\n",
    "    def regulate_obs(self, obs: np.ndarray,time_step:int = None)-> np.ndarray:\n",
    "        float_num = obs // self.increment + (self.num_of_increments //2)\n",
    "        float_num = float_num[:-2]\n",
    "        bool_num = obs[-2:]\n",
    "\n",
    "        low_limit = np.zeros(8)\n",
    "        high_limit = self.num_of_increments -1\n",
    "\n",
    "        reg_obs = np.concatenate((float_num,bool_num)).astype(int)\n",
    "        reg_obs = reg_obs.clip(low_limit,high_limit).astype(int)\n",
    "        if time_step:\n",
    "            return self.add_time_dim(reg_obs,time_step)\n",
    "        else:\n",
    "            return reg_obs\n",
    "    \n",
    "    def add_time_dim(self,obs:np.ndarray, time_step: int):\n",
    "        time_incr = 20\n",
    "        time_step = min(4,time_step//time_incr)\n",
    "        return np.insert(obs,0,time_step)\n",
    "\n",
    "\n",
    "def create_Q_table(*dim):\n",
    "    return np.zeros(dim)\n",
    "\n",
    "#Bellman equation, Q\n",
    "def Q_observed(s2_reward:np.float64, s2_obs: np.ndarray, q_table: np.ndarray, gamma:float):\n",
    "    arg_max_Q = np.argmax(q_table[tuple(s2_obs)])\n",
    "    return s2_reward + gamma * arg_max_Q\n",
    "\n",
    "def Q_observed(reward:np.float64, s2_obs: np.ndarray, q_table: np.ndarray, gamma:float):\n",
    "    arg_max_Q = np.argmax(q_table[tuple(s2_obs)])\n",
    "    return reward + gamma * arg_max_Q\n",
    "\n",
    "def Q_expected(s1_obs: np.ndarray,s1_action: np.int64, q_table: np.ndarray):\n",
    "    index = *s1_obs , s1_action\n",
    "    return q_table[index]\n",
    "\n",
    "def update_Q(Q_observed: float,Q_expected: float, alpha: float, q_table: np.ndarray,s1_obs: np.ndarray,s1_action: np.int64):\n",
    "    index = *s1_obs , s1_action\n",
    "    q_table[index] = Q_expected + alpha * (Q_observed - Q_expected)\n",
    "    return q_table\n",
    "\n",
    "def action_policy(q_table: np.ndarray, s1_obs: np.ndarray):\n",
    "    return np.argmax(q_table[tuple(s1_obs)])\n",
    "\n",
    "def reform_reward(reward,observation):\n",
    "    # min_y_speed = -0.7\n",
    "    # if observation[3] < min_y_speed:\n",
    "    #     y_speed=observation[3]\n",
    "    # else:\n",
    "    #     y_speed = 0\n",
    "\n",
    "    # min_x_speed = 1.5\n",
    "    # if abs(observation[3]) < min_x_speed:\n",
    "    #     y_speed=abs(observation[3])\n",
    "    # else:\n",
    "    #     y_speed = 0\n",
    "    # reward = reward - 2.5*(y_speed**2) -5*(observation[5]**2) - 2.5*(y_speed**2)\n",
    "    # # - 5*((observation[1]-1.5)*observation[3])\n",
    "    return reward\n",
    "\n",
    "def expo_decay(large_epsilon, small_epsilon, epoch, steps):\n",
    "    a = large_epsilon\n",
    "    b = small_epsilon\n",
    "    e = np.e\n",
    "    z = 1- steps/epoch\n",
    "    return z*((a-b)/e)*(e**z)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_incre = 5\n",
    "load_data = True\n",
    "states = np.array([10,10,10,10,10,10,2,2])\n",
    "if load_data:\n",
    "    q_table = np.load(\"q_table_60M.npy\")\n",
    "else:\n",
    "    q_table = create_Q_table(*states,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "observation, info = env.reset()\n",
    "max_alpha = 0.1\n",
    "min_alpha = 0.0001\n",
    "# max_alpha = 0.01\n",
    "# min_alpha = 0.01\n",
    "gamma = 0.55\n",
    "max_epsilon = 0.3\n",
    "min_epsilon = 0.01\n",
    "# max_epsilon = 0.1\n",
    "# min_epsilon = 0.1\n",
    "epoch = 60_000_000\n",
    "\n",
    "agent = QLearn(env)\n",
    "agent.set_num_of_increments(states)\n",
    "game_step = 0\n",
    "obs = agent.regulate_obs(observation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26206/60000000 [00:10<6:22:35, 2612.61it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for _ in tqdm(range(epoch)):\n",
    "        epsilon = expo_decay(max_epsilon,min_epsilon,epoch,_)\n",
    "        alpha = expo_decay(max_alpha,min_alpha,epoch,_)\n",
    "        policy_action = action_policy(q_table,obs)\n",
    "        action = policy_action if random.random() > epsilon else env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        new_obs = agent.regulate_obs(observation)\n",
    "        reward = reform_reward(reward,observation)\n",
    "\n",
    "        q_observe = Q_observed(reward,new_obs,q_table,gamma)\n",
    "        q_expected = Q_expected(obs,action,q_table)\n",
    "\n",
    "        update_Q(q_observe,q_expected,alpha,q_table,obs,action)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            obs = agent.regulate_obs(observation)\n",
    "        else:\n",
    "            obs = new_obs\n",
    "except KeyboardInterrupt:\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playsound import playsound\n",
    "playsound('./yakemashita.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Render Lunar Lander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    env = gym.make(\"LunarLander-v2\",render_mode=\"human\")\n",
    "    observation, info = env.reset()\n",
    "    obs = agent.regulate_obs(observation)\n",
    "    reward = 0\n",
    "\n",
    "    while True:\n",
    "        game_step += 1\n",
    "        action = action_policy(q_table,obs)\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        new_obs = agent.regulate_obs(observation)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            game_step = 0\n",
    "            obs = agent.regulate_obs(observation)\n",
    "        else:\n",
    "            obs = new_obs\n",
    "except KeyboardInterrupt:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check surivial rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test\n",
      "term: 635, survival rate: 0.08976377952755905"
     ]
    }
   ],
   "source": [
    "print(\"Start test\")\n",
    "try:\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    obs = agent.regulate_obs(observation,0)\n",
    "\n",
    "\n",
    "    term_count = 0\n",
    "    survival_count = 0\n",
    "    for _ in range(1000000):\n",
    "        game_step += 1\n",
    "\n",
    "        policy_action = action_policy(q_table,obs)\n",
    "        action = policy_action\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        new_obs = agent.regulate_obs(observation)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            observation, info = env.reset()\n",
    "            game_step = 0\n",
    "            obs = agent.regulate_obs(observation)\n",
    "            term_count += 1\n",
    "            if reward > 99:\n",
    "                survival_count +=1\n",
    "                print(f\"\\rterm: {term_count}, survival rate: {survival_count/term_count}\",end=\"\")\n",
    "        else:\n",
    "            obs = new_obs\n",
    "    print(f\"Final rate {100000/term_count}\")\n",
    "    env.close()\n",
    "except KeyboardInterrupt:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./q_table_60M\",q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Wrapper.close of <TimeLimit<OrderEnforcing<PassiveEnvChecker<LunarLander<LunarLander-v2>>>>>>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforce_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
