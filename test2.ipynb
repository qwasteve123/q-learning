{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "def temp(r):\n",
    "    return 0.01 + r\n",
    "\n",
    "env = gym.wrappers.TransformReward(env, temp)\n",
    "\n",
    "env = gym.RewardWrapper(env)\n",
    "observation, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# agent policy that uses the observation and info\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(observation, reward)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\Users\\waito\\Desktop\\Python_projects\\Reinforcement\\reinforce_env\\lib\\site-packages\\gymnasium\\core.py:556\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[0;32m    555\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m--> 556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\waito\\Desktop\\Python_projects\\Reinforcement\\reinforce_env\\lib\\site-packages\\gymnasium\\core.py:567\u001b[0m, in \u001b[0;36mRewardWrapper.reward\u001b[1;34m(self, reward)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreward\u001b[39m(\u001b[38;5;28mself\u001b[39m, reward: SupportsFloat) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SupportsFloat:\n\u001b[0;32m    559\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified environment ``reward``.\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \n\u001b[0;32m    561\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03m        The modified `reward`\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "print(observation, reward)\n",
    "\n",
    "if terminated or truncated:\n",
    "    observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Wrapper.close of <RewardWrapper<TransformReward<TimeLimit<OrderEnforcing<PassiveEnvChecker<LunarLander<LunarLander-v2>>>>>>>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax(np.array([0,0,1,0]))\n",
    "np.all(np.array([0,0,00]) == 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforce_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
